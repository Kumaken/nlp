{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Abel Stanley\n",
    "\n",
    "NIM: 13517068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'you', ',', 'you', 'love', 'me', ',', 'we', \"'re\", 'happy', 'fat', 'maw', 'leek', '.', 'With', 'a', 'big-big', 'U.K.', 'buck', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "sentence = \"I love you, you love me, we're happy fat maw leek. With a big-big U.K. buck.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, love, you,, you, love, me,, we're, happy, fat, maw, leek., With, a, big-big, U.K., buck.]\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = tokenizer(sentence)\n",
    "print(list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples >> apple\n",
      "and >> and\n",
      "oranges >> orange\n",
      "are >> are\n",
      "similar >> similar\n",
      ". >> .\n",
      "Boots >> boot\n",
      "and >> and\n",
      "hippos >> hippo\n",
      "are >> are\n",
      "n't >> n't\n",
      ". >> .\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "# Setups:\n",
    "import nltk\n",
    "# nltk.download('wordnet') # download first!\n",
    "sentence = u\"Apples and oranges are similar. Boots and hippos aren't.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "for word in tokens:\n",
    "    print(word, \">>\", lemmatizer.lemmatize(word.lower()))\n",
    "    \n",
    "# NOTE:\n",
    "# for nltk Lemmatization to work properly, the word has to be in lowercase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Apples | Lemma: 8566208034543834098 | Lemma_: apple\n",
      "Token: and | Lemma: 2283656566040971221 | Lemma_: and\n",
      "Token: oranges | Lemma: 2208928596161743350 | Lemma_: orange\n",
      "Token: are | Lemma: 10382539506755952630 | Lemma_: be\n",
      "Token: similar | Lemma: 18166476740537071113 | Lemma_: similar\n",
      "Token: . | Lemma: 12646065887601541794 | Lemma_: .\n",
      "Token: Boots | Lemma: 9918665227421442029 | Lemma_: boot\n",
      "Token: and | Lemma: 2283656566040971221 | Lemma_: and\n",
      "Token: hippos | Lemma: 4133693291145879083 | Lemma_: hippos\n",
      "Token: are | Lemma: 10382539506755952630 | Lemma_: be\n",
      "Token: n't | Lemma: 447765159362469301 | Lemma_: not\n",
      "Token: . | Lemma: 12646065887601541794 | Lemma_: .\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Token:\", token, \"| Lemma:\", token.lemma, \"| Lemma_:\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE WASHINGTON\n",
      "GPE New York\n",
      "PERSON Loretta E. Lynch\n",
      "GPE Brooklyn\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "# Setups:\n",
    "import nltk\n",
    "# DOWNLOAD:\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "for sent in nltk.sent_tokenize(sentence):\n",
    "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "         print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: WASHINGTON | Start: 0 | End: 10 | Label: GPE\n",
      "Text: New York | Start: 51 | End: 59 | Label: GPE\n",
      "Text: the 1990s | Start: 79 | End: 88 | Label: DATE\n",
      "Text: Loretta E. Lynch | Start: 90 | End: 106 | Label: PERSON\n",
      "Text: Brooklyn | Start: 138 | End: 146 | Label: GPE\n",
      "Text: African-Americans | Start: 203 | End: 220 | Label: NORP\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\"Text:\", ent.text, \"| Start:\", ent.start_char, \"| End:\", ent.end_char, \"| Label:\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WASHINGTON', 'NNP'),\n",
       " ('--', ':'),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wake', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('string', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('abuses', 'NNS'),\n",
       " ('by', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('police', 'NN'),\n",
       " ('officers', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('1990s', 'CD'),\n",
       " (',', ','),\n",
       " ('Loretta', 'NNP'),\n",
       " ('E.', 'NNP'),\n",
       " ('Lynch', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('top', 'JJ'),\n",
       " ('federal', 'JJ'),\n",
       " ('prosecutor', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Brooklyn', 'NNP'),\n",
       " (',', ','),\n",
       " ('spoke', 'VBD'),\n",
       " ('forcefully', 'RB'),\n",
       " ('about', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pain', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('broken', 'JJ'),\n",
       " ('trust', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('African-Americans', 'NNP'),\n",
       " ('felt', 'VBD'),\n",
       " ('and', 'CC'),\n",
       " ('said', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('responsibility', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('repairing', 'VBG'),\n",
       " ('generations', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('miscommunication', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('mistrust', 'NN'),\n",
       " ('fell', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('law', 'NN'),\n",
       " ('enforcement', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "# Setups:\n",
    "import nltk\n",
    "# DOWNLOAD:\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "text = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', ':', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNP', 'NN', 'NNS', 'IN', 'DT', 'NNS', ',', 'NNP', 'NNP', 'NNP', ',', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNP', ',', 'VBD', 'RB', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'HYPH', 'NNPS', 'VBD', 'CC', 'VBD', 'DT', 'NN', 'IN', 'VBG', 'NNS', 'IN', 'NN', 'CC', 'NN', 'VBD', 'IN', 'NN', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "pos_tagging_result = [token.tag_ for token in doc]\n",
    "print(pos_tagging_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK (Grammar Parser + Chart Parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> V NP | S CONJ S\n",
    "V -> 'describe' | 'present'\n",
    "NP -> PRP N | DT N PP | DT N | ADJ N PP | DT NP\n",
    "PRP -> 'your' \n",
    "N -> 'work' | 'step' | 'results' | 'Word_Document'\n",
    "PP -> P NP\n",
    "P -> 'of' | 'in'\n",
    "DT -> 'every' | 'a' | 'all'\n",
    "ADJ -> 'final' | 'intermediate' | ADJ CONJ ADJ\n",
    "CONJ -> 'and'\n",
    "\"\"\")\n",
    "\n",
    "grammar.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> V NP,\n",
       " S -> S CONJ S,\n",
       " V -> 'describe',\n",
       " V -> 'present',\n",
       " NP -> PRP N,\n",
       " NP -> DT N PP,\n",
       " NP -> DT N,\n",
       " NP -> ADJ N PP,\n",
       " NP -> DT NP,\n",
       " PRP -> 'your',\n",
       " N -> 'work',\n",
       " N -> 'step',\n",
       " N -> 'results',\n",
       " N -> 'Word_Document',\n",
       " PP -> P NP,\n",
       " P -> 'of',\n",
       " P -> 'in',\n",
       " DT -> 'every',\n",
       " DT -> 'a',\n",
       " DT -> 'all',\n",
       " ADJ -> 'final',\n",
       " ADJ -> 'intermediate',\n",
       " ADJ -> ADJ CONJ ADJ,\n",
       " CONJ -> 'and']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chart Parsing in NLTK\n",
      "[Tree('S', [Tree('V', ['present']), Tree('NP', [Tree('DT', ['all']), Tree('NP', [Tree('ADJ', [Tree('ADJ', ['intermediate']), Tree('CONJ', ['and']), Tree('ADJ', ['final'])]), Tree('N', ['results']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('DT', ['a']), Tree('N', ['Word_Document'])])])])])])]\n"
     ]
    }
   ],
   "source": [
    "print('Chart Parsing in NLTK')\n",
    "sentence = 'present all intermediate and final results in a Word_Document'.split()\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = list(parser.parse(sentence))\n",
    "\n",
    "print(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I nsubj love\n",
      "you you dobj love\n",
      "you you nsubj love\n",
      "me me dobj love\n",
      "we we nsubj 're\n",
      "happy fat maw leek leek attr 're\n",
      "a big-big U.K. buck buck pobj With\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"I love you, you love me, we're happy fat maw leek. With a big-big U.K. buck.\"\n",
    "doc = nlp(sentence)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitions of 'bank':\n",
      "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') a long ridge or pile\n",
      "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') a building in which the business of banking transacted\n",
      "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') tip laterally\n",
      "Synset('bank.v.02') enclose with a bank\n",
      "Synset('bank.v.03') do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') be in the banking business\n",
      "Synset('deposit.v.02') put into a bank account\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "print(\"definitions of 'bank':\")\n",
    "for ss in nltk.corpus.wordnet.synsets('bank'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "sentence = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "# disambiguate the word \"bank\"\n",
    "print(nltk.wsd.lesk(sentence, 'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy does not have direct support for Word Sense Disambiguation\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "# NOT SUPPORTED\n",
    "print(\"spaCy does not have direct support for Word Sense Disambiguation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "# DOWNLOADS:\n",
    "# nltk.download('subjectivity')\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sentence, 'subj') for sentence in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sentence, 'obj') for sentence in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "print(len(subj_docs), len(obj_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example docs content:\n",
      "(['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj')\n"
     ]
    }
   ],
   "source": [
    "print(\"Example docs content:\")\n",
    "print(subj_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:\n"
     ]
    }
   ],
   "source": [
    "print(\"Separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets.\")\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  simple unigram word features, handling negation:\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply features to obtain a feature-value representation of our datasets:\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "# train classifier on the training set, and subsequently output the evaluation results:\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "# train classifier:\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Topic(Enum):\n",
    "    AMBIENCE = 1\n",
    "    FOOD = 2\n",
    "    HYGIENE = 3\n",
    "    SERVICE = 4\n",
    "    VALUE = 5\n",
    "    \n",
    "class Rating(Enum):\n",
    "    VERY_BAD = -3\n",
    "    BAD = -2\n",
    "    SOMEWHAT_BAD = -1\n",
    "    SOMEWHAT_GOOD = 1\n",
    "    GOOD = 2\n",
    "    VERY_GOOD = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Entry\n",
    "- mostly a data container\n",
    "- but we also want to be able to compare if it matches a spaCy Token --> we need a matching() function.\n",
    "- tokens can match exactly or only after transformations (for example upper/lower case) --> score between 0 (no match) and 1 (perfect match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LexiconEntry:\n",
    "    _IS_REGEX_REGEX = re.compile(r'.*[.+*\\[$^\\\\]')\n",
    "\n",
    "    def __init__(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        assert lemma is not None\n",
    "        self.lemma = lemma\n",
    "        self._lower_lemma = lemma.lower()\n",
    "        self.topic = topic\n",
    "        self.rating = rating\n",
    "        self.is_regex = bool(LexiconEntry._IS_REGEX_REGEX.match(self.lemma))\n",
    "        self._regex = re.compile(lemma, re.IGNORECASE) if self.is_regex else None\n",
    "\n",
    "    def matching(self, token: Token) -> float:\n",
    "        \"\"\"\n",
    "        A weight between 0.0 and 1.0 on how much ``token`` matches this entry.\n",
    "        \"\"\"\n",
    "        assert token is not None\n",
    "        result = 0.0\n",
    "        if self.is_regex:\n",
    "            if self._regex.match(token.text):\n",
    "                result = 0.6\n",
    "            elif self._regex.match(token.lemma_):\n",
    "                result = 0.5\n",
    "        else:\n",
    "            if token.text == self.lemma:\n",
    "                result = 1.0\n",
    "            elif token.text.lower() == self.lemma:\n",
    "                result = 0.9\n",
    "            elif token.lemma_ == self.lemma:\n",
    "                result = 0.8\n",
    "            elif token.lemma_.lower() == self.lemma:\n",
    "                result = 0.7\n",
    "        return result\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        result = 'LexiconEntry(%s' % self.lemma\n",
    "        if self.topic is not None:\n",
    "            result += ', topic=%s' % self.topic.name\n",
    "        if self.rating is not None:\n",
    "            result += ', rating=%s' % self.rating.name\n",
    "        if self.is_regex:\n",
    "            result += ', is_regex=%s' % self.is_regex\n",
    "        result += ')'\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The lexicon in Python\n",
    "- Contains a list of LexiconEntry\n",
    "- Can find the best matching entry for a Token (or None)\n",
    "- In the beginning entries have to be added\n",
    "- usually data comes from e.g .CSV (NOT MANUALLY INSERTED LIKE NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from math import isclose\n",
    "\n",
    "class Lexicon:\n",
    "    def __init__(self):\n",
    "        self.entries: List[LexiconEntry] = []\n",
    "\n",
    "    \n",
    "    def append(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        lexicon_entry = LexiconEntry(lemma, topic, rating)\n",
    "        self.entries.append(lexicon_entry)\n",
    "\n",
    "    def lexicon_entry_for(self, token: Token) -> LexiconEntry:\n",
    "        \"\"\"\n",
    "        Entry in lexicon that best matches ``token``.\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        lexicon_size = len(self.entries)\n",
    "        lexicon_entry_index = 0\n",
    "        best_matching = 0.0\n",
    "        while lexicon_entry_index < lexicon_size and not isclose(best_matching, 1.0):\n",
    "            lexicon_entry = self.entries[lexicon_entry_index]\n",
    "            matching = lexicon_entry.matching(token)\n",
    "            if matching > best_matching:\n",
    "                result = lexicon_entry\n",
    "                best_matching = matching\n",
    "            lexicon_entry_index += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build a small lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lexicon = Lexicon()\n",
    "lexicon.append('waiter'     , Topic.SERVICE , None)\n",
    "lexicon.append('waitress'   , Topic.SERVICE , None)\n",
    "lexicon.append('wait'       , None          , Rating.BAD)\n",
    "lexicon.append('quick'      , None          , Rating.GOOD)\n",
    "lexicon.append('.*schnitzel', Topic.FOOD    , None)\n",
    "lexicon.append('music'      , Topic.AMBIENCE, None)\n",
    "lexicon.append('loud'       , None          , Rating.BAD)\n",
    "lexicon.append('tasty'      , Topic.FOOD    , Rating.GOOD)\n",
    "lexicon.append('polite'     , Topic.SERVICE , Rating.GOOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching tokens in a sentence to a lexicon entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        None\n",
      "music      LexiconEntry(music, topic=AMBIENCE)\n",
      "was        None\n",
      "very       None\n",
      "loud       LexiconEntry(loud, rating=BAD)\n",
      ".          None\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp(feedback_text)\n",
    "for token in next(feedback.sents):\n",
    "    lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "    print(f'{token!s:10} {lexicon_entry}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sentiment Analysis: Add filters and format to ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The music was very loud.\n",
      "     Topic.AMBIENCE\n",
      "     Rating.BAD\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp(feedback_text)\n",
    "for sent in feedback.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "        if lexicon_entry is not None:\n",
    "            if lexicon_entry.topic is not None:\n",
    "                print('    ', lexicon_entry.topic)\n",
    "            if lexicon_entry.rating is not None:\n",
    "                print('    ', lexicon_entry.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Sentence Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I love you, you love me, we're happy fat maw leek.\", 'With a big-big U.K. buck.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love you, you love me, we're happy fat maw leek. With a big-big U.K. buck.\"\n",
    "print(nltk.tokenize.sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: as you can see, NLTK succeeds to differentiate dots in initials like (U.K.) from full-stop dots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I love you, you love me, we're happy fat maw leek., With a big-big U.K. buck.]\n"
     ]
    }
   ],
   "source": [
    "# spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"I love you, you love me, we're happy fat maw leek. With a big-big U.K. buck.\"\n",
    "doc = nlp(sentence)\n",
    "    \n",
    "splitted_result = list(doc.sents)\n",
    "print(splitted_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: as you can see, spaCy fails to differentiate dots in initials like (U.K.) from full-stop dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "a65c17a7-e1ca-4afb-b7a7-bcf261eb3d3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
